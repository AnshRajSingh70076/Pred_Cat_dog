# -*- coding: utf-8 -*-
"""Untitled23.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-eJboFFZi5svOi6FO43bTEj9BnuZXzqw
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
from tensorflow.keras import layers,models
import matplotlib.pyplot as plt

dataset=tf.keras.preprocessing.image_dataset_from_directory(
          "/content/drive/MyDrive/Colab Notebooks/TEST_DATA"  ,
          shuffle=True  ,
          seed=123,
          batch_size=1
)

!pip install keras-tuner

class_name=dataset.class_names
class_name

def resizer(image,label):
  image=tf.image.resize(image,(255,255))
  return image,label

len(dataset)

dataset = dataset.map(lambda image, label: resizer(image, label))
dataset = dataset.batch(100)

def get_Train_test_data(ds,train_size=0.8,val_size=0.1,test_size=0.1,shuffle=True,shuffle_size=10000):
    train_size1=int(len(ds)*train_size)
    if shuffle:
        ds = ds.shuffle(shuffle_size, seed=12)
    train_ds=ds.take(train_size1)
    val_size1=int(len(ds)*val_size)
    val_ds=ds.skip(train_size1).take(val_size1)
    test_ds=ds.skip(train_size1).skip(val_size1)

    return train_ds,val_ds,test_ds

train_ds,val_ds,test_ds=get_Train_test_data(dataset)

len(dataset)

import tensorflow as tf

# Load your dataset (replace with your dataset loading code)
dataset = tf.keras.preprocessing.image_dataset_from_directory(
    "/content/drive/MyDrive/Colab Notebooks/TEST_DATA",
    # image_size=(height, width),  # Adjust height and width as needed
    batch_size=1  # Use batch size of 1 to inspect individual images
)

# Take a batch from the dataset
for images, labels in dataset.take(1):
    image = images[0].numpy()  # Get the first image in the batch
    print(f'Image shape: {image.shape}')
    print(f'Image pixel values: \n{image}')  # Print pixel values
    print(f'Label: {labels[0].numpy()}')  # Print the corresponding label

def resize_image(image, label, height, width):
    # Ensure the image has the correct number of dimensions
    if len(image.shape) == 4:  # If there's a batch dimension
        image = tf.squeeze(image, axis=0)  # Remove the batch dimension if it exists

    # Resize the image
    image = tf.image.resize(image, [height, width])

    # Convert image to float32 format
    image = tf.image.convert_image_dtype(image, tf.float32)

    return image, label

# Apply resizing
dataset = dataset.map(lambda image, label: resize_image(image, label, height, width))

len(dataset)

dataset=dataset.batch(100)

!pip install keras-tuner

def get_Train_test_data(ds,train_size=0.8,val_size=0.1,test_size=0.1,shuffle=True,shuffle_size=10000):
    train_size1=int(len(ds)*train_size)
    if shuffle:
        ds = ds.shuffle(shuffle_size, seed=12)
    train_ds=ds.take(train_size1)
    val_size1=int(len(ds)*val_size)
    val_ds=ds.skip(train_size1).take(val_size1)
    test_ds=ds.skip(train_size1).skip(val_size1)

    return train_ds,val_ds,test_ds

train_ds,val_ds,test_ds=get_Train_test_data(dataset)

len(train_ds)

train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)
val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)
test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)

def preprocess_image(image, label):
    # Remove any extra dimensions
    image = tf.squeeze(image)

    # Resize image to the expected dimensions
    image = tf.image.resize(image, [255, 255])

    return image, label

def preprocess_image(image, label):
    image = tf.image.resize(image, [255, 255])
    image = tf.image.convert_image_dtype(image, tf.float32)  # Ensure dtype is float32
    return image, label

# Apply preprocessing
train_ds = (train_ds
            .map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
            .cache()
            .shuffle(1000)
            .batch(32)
            .prefetch(buffer_size=tf.data.AUTOTUNE))

val_ds = (val_ds
          .map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
          .cache()
          .batch(32)
          .prefetch(buffer_size=tf.data.AUTOTUNE))

test_ds = (test_ds
           .map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
           .cache()
           .batch(32)
           .prefetch(buffer_size=tf.data.AUTOTUNE))

import tensorflow as tf
from tensorflow.keras import layers
from kerastuner import RandomSearch

def build_model(hp):
    model = tf.keras.Sequential()

    # Hyperparameters for convolutional layers
    for i in range(hp.Int('num_conv_layers', 1, 3)):
        if i == 0:
            model.add(layers.Conv2D(
                filters=hp.Int('filters_' + str(i), min_value=32, max_value=128, step=32),
                kernel_size=hp.Choice('kernel_size_' + str(i), values=[3, 5]),
                activation='relu',
                input_shape=(255, 255, 3)  # Input shape for the first layer
            ))
        else:
            model.add(layers.Conv2D(
                filters=hp.Int('filters_' + str(i), min_value=32, max_value=128, step=32),
                kernel_size=hp.Choice('kernel_size_' + str(i), values=[3, 5]),
                activation='relu'
            ))
        model.add(layers.MaxPooling2D(pool_size=2))

    model.add(layers.Flatten())

    # Hyperparameters for dense layers
    for i in range(hp.Int('num_dense_layers', 1, 2)):
        model.add(layers.Dense(
            units=hp.Int('units_' + str(i), min_value=64, max_value=256, step=64),
            activation='relu'
        ))

    model.add(layers.Dense(10, activation='softmax'))  # Assuming 10 classes

    # Compile model
    model.compile(
        optimizer=tf.keras.optimizers.Adam(
            hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])
        ),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

# Set up Keras Tuner
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='my_dir',
    project_name='cnn_tuning',
    overwrite=True  # Overwrite existing files
)

# Ensure data is preprocessed correctly
train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)
val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)
test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)

# Start the tuning process
tuner.search(
    train_ds,
    epochs=10,
    validation_data=val_ds
)

# Get the best model and summarize
best_model = tuner.get_best_models(num_models=1)[0]
best_model.summary()

# Evaluate on the test set
test_loss, test_acc = best_model.evaluate(test_ds)
print(f"Test accuracy: {test_acc:.4f}")

import tensorflow as tf
from tensorflow.keras import layers, models

def build_model():
    model = models.Sequential()

    # Convolutional layers
    model.add(layers.Conv2D(32, (5, 5), activation='relu', input_shape=(255, 255, 3)))
    model.add(layers.MaxPooling2D(pool_size=(2, 2)))

    model.add(layers.Conv2D(32, (5, 5), activation='relu'))
    model.add(layers.MaxPooling2D(pool_size=(2, 2)))

    model.add(layers.Conv2D(128, (5, 5), activation='relu'))
    model.add(layers.MaxPooling2D(pool_size=(2, 2)))

    model.add(layers.Flatten())

    # Dense layers
    model.add(layers.Dense(256, activation='relu'))
    model.add(layers.Dense(10, activation='softmax'))  # Assuming 10 classes

    # Compile the model
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

# Build the model
model = build_model()

# Summary of the model
model.summary()

# Fit the model
history = model.fit(
    train_ds,
    epochs=10,
    validation_data=val_ds
)

import tensorflow as tf

def resize_image(image, label, height, width):
    image = tf.image.resize(image, [height, width])
    return image, label

def preprocess_dataset(dataset, height, width):
    dataset = dataset.map(lambda image, label: resize_image(image, label, height, width))
    dataset = dataset.batch(32)  # Adjust batch size if needed
    dataset = dataset.prefetch(tf.data.AUTOTUNE)
    return dataset

train_ds = preprocess_dataset(train_ds, 255, 255)
val_ds = preprocess_dataset(val_ds, 255, 255)

for images, labels in train_ds.take(1):
    print(images.shape)  # Should print (batch_size, 255, 255, 3)

pip install keras-tuner

from kerastuner.tuners import RandomSearch

tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=2,  # Number of different hyperparameter combinations to try
    executions_per_trial=3,  # Number of times to train each model
    directory='my_dir',
    project_name='cnn_tuning'
)

tuner.search(
    train_ds,
    epochs=2,
    validation_data=val_ds
)

import tensorflow as tf

# Define the target size
target_height = 255
target_width = 255

# Load dataset with no resizing
dataset = tf.keras.preprocessing.image_dataset_from_directory(
    "/content/drive/MyDrive/Colab Notebooks/TEST_DATA",
    shuffle=True,
    seed=123,
    batch_size=32,  # Adjust as needed
    image_size=(target_height, target_width)  # This is to ensure consistent resizing
)

# Preprocessing function to resize images and normalize pixel values
def preprocess_image(image, label):
    image = tf.image.resize(image, [target_height, target_width])
    image = image / 255.0  # Normalize pixel values to [0, 1]
    return image, label

# Apply preprocessing
dataset = dataset.map(preprocess_image)

# Batch and prefetch the dataset for performance
dataset = dataset.batch(32)  # Adjust batch size as needed
dataset = dataset.prefetch(tf.data.AUTOTUNE)

# Verify the shape of the dataset
for image_batch, label_batch in dataset.take(1):
    print(image_batch.shape)  # Should be (batch_size, target_height, target_width, 3)

import tensorflow as tf

# Define the target size
target_height = 255
target_width = 255

# Load dataset with resizing
dataset = tf.keras.preprocessing.image_dataset_from_directory(
    "/content/drive/MyDrive/Colab Notebooks/TEST_DATA",
    shuffle=True,
    seed=123,
    batch_size=32,  # Adjust as needed
    image_size=(target_height, target_width)  # Ensure consistent resizing
)

# Preprocessing function to normalize pixel values
def preprocess_image(image, label):
    image = image / 255.0  # Normalize pixel values to [0, 1]
    return image, label

# Apply preprocessing
dataset = dataset.map(preprocess_image)

# Verify the shape of the dataset
for image_batch, label_batch in dataset.take(1):
    print(image_batch.shape)  # Should be (batch_size, target_height, target_width, 3)

def preprocess_image(image, label):
    image = image / 255.0  # Normalize pixel values to [0, 1]
    return image, label

dataset = dataset.map(preprocess_image)

# Split the dataset
dataset_size = len(dataset)
train_size = int(0.8 * dataset_size)
val_size = dataset_size - train_size

train_ds = dataset.take(train_size)
val_ds = dataset.skip(train_size).take(val_size)

import tensorflow as tf
from tensorflow.keras import layers
import keras_tuner as kt

def build_model(hp):
    model = tf.keras.Sequential()

    # Define the number of convolutional layers
    num_conv_layers = hp.Int('num_conv_layers', min_value=1, max_value=3, step=1)

    for i in range(num_conv_layers):
        filters = hp.Int(f'filters_{i}', min_value=32, max_value=128, step=32)
        kernel_size = hp.Choice(f'kernel_size_{i}', values=[3, 5])
        model.add(layers.Conv2D(filters, (kernel_size, kernel_size), activation='relu', padding='same'))
        model.add(layers.MaxPooling2D((2, 2)))

    model.add(layers.Flatten())

    # Define the number of dense layers
    num_dense_layers = hp.Int('num_dense_layers', min_value=1, max_value=2, step=1)

    for i in range(num_dense_layers):
        units = hp.Int(f'units_{i}', min_value=64, max_value=256, step=64)
        model.add(layers.Dense(units, activation='relu'))

    model.add(layers.Dense(2, activation='softmax'))  # Assuming binary classification

    model.compile(
        optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

tuner = kt.RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=3,  # Number of hyperparameter combinations to try
    executions_per_trial=1,  # Number of times to train each model
    directory='my_dir',  # Directory to save tuning results
    project_name='intro_to_kt'
)

# Define the dataset preprocessing function
def preprocess_image(image, label):
    image = image / 255.0  # Normalize pixel values to [0, 1]
    return image, label

# Apply preprocessing to dataset
train_ds = train_ds.map(preprocess_image).cache().prefetch(buffer_size=tf.data.AUTOTUNE)
val_ds = val_ds.map(preprocess_image).cache().prefetch(buffer_size=tf.data.AUTOTUNE)

# Start the tuning process
tuner.search(
    train_ds,
    validation_data=val_ds,
    epochs=4
)